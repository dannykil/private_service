"""
LLM ì„œë¹™ ì„œë²„ (í¬íŠ¸ 8001)
- LLM ê°ì²´ë¥¼ ë©”ëª¨ë¦¬ì— ìœ ì§€í•˜ì—¬ ì½œë“œìŠ¤íƒ€íŠ¸ ë°©ì§€
- í•œ ë²ˆ ì‹¤í–‰ í›„ ê³„ì† ë„ì›Œë‘ 
"""
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from langchain_ollama import OllamaLLM
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from contextlib import asynccontextmanager
from datetime import datetime
import os

# LLM ê°ì²´ë¥¼ ì „ì—­ìœ¼ë¡œ ìœ ì§€
llm = None
chain = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì„œë²„ ì‹œì‘ ì‹œ LLM ì´ˆê¸°í™” (ì½œë“œìŠ¤íƒ€íŠ¸ 1íšŒ)"""
    global llm, chain
    
    print("ğŸš€ LLM ì„œë¹™ ì„œë²„ ì‹œì‘ - ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ Ollama URL ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’: localhost)
    ollama_base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    print(f"ğŸ”— Ollama ì„œë²„ ì—°ê²°: {ollama_base_url}")
    
    # LLM ì´ˆê¸°í™”
    llm = OllamaLLM(
        # model="gemma2:9b",
        model="gemma2:2b",
        base_url=ollama_base_url,
        # num_predict=256,  # ì‘ë‹µ í† í° ìˆ˜ ì œí•œ (ê¸°ë³¸ê°’ ë¬´ì œí•œ â†’ 256)
        num_predict=128,  # ì‘ë‹µ í† í° ìˆ˜ ì œí•œ (ê¸°ë³¸ê°’ ë¬´ì œí•œ â†’ 256)
    )
    
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ + ì²´ì¸ êµ¬ì„±
    template = """Question: {question}

Answer: Let's think step by step."""
    prompt = PromptTemplate(template=template, input_variables=["question"])
    chain = prompt | llm | StrOutputParser()
    
    # ì›œì—… ìš”ì²­ (ì„ íƒì‚¬í•­ - ì²« ìš”ì²­ ì§€ì—° ë°©ì§€)
    print("ğŸ”¥ ì›œì—… ìš”ì²­ ì‹¤í–‰ ì¤‘...")
    try:

        start_time = datetime.now()

        # _ = llm.invoke("Hello")
        answer = llm.invoke("Hello")

        end_time = datetime.now()
        duration = end_time - start_time

        # print("âœ… LLM ì›œì—… ì™„ë£Œ! ì„œë²„ ì¤€ë¹„ë¨")
        print(f"âœ… LLM ì›œì—… ì™„ë£Œ! ì„œë²„ ì¤€ë¹„ë¨: {answer}")

        print(f"â±ï¸  í˜¸ì¶œì‹œê°„: {start_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
        print(f"â±ï¸  ì‘ë‹µì‹œê°„: {end_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
        print(f"â±ï¸  ê±¸ë¦°ì‹œê°„: {duration.total_seconds():.3f}ì´ˆ")

    except Exception as e:
        print(f"âš ï¸ ì›œì—… ì‹¤íŒ¨ (Ollama ì„œë²„ í™•ì¸ í•„ìš”): {e}")
    
    yield  # ì„œë²„ ì‹¤í–‰
    
    # ì„œë²„ ì¢…ë£Œ ì‹œ
    print("ğŸ‘‹ LLM ì„œë¹™ ì„œë²„ ì¢…ë£Œ")

app = FastAPI(
    title="LLM Serving Server",
    description="Ollama LLM ì„œë¹™ ì„œë²„ - ì½œë“œìŠ¤íƒ€íŠ¸ ë°©ì§€ìš©",
    lifespan=lifespan
)

# ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
class ChatRequest(BaseModel):
    question: str
    use_chain: bool = False  # Trueë©´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©

class ChatResponse(BaseModel):
    answer: str
    # model: str = "gemma2:9b"
    model: str = "gemma2:2b"
    

# í—¬ìŠ¤ì²´í¬
@app.get("/health")
async def health_check():
    print(f"LLM í—¬ìŠ¤ì²´í¬ ìš”ì²­ ë°›ìŒ")

    return {"status": "ok", "model_loaded": llm is not None}

# ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸
@app.post("/chat", response_model=ChatResponse)
def chat(request: ChatRequest):  # async ì œê±° â†’ FastAPIê°€ ìŠ¤ë ˆë“œí’€ì—ì„œ ì‹¤í–‰
    print(f"LLM ì±„íŒ… ìš”ì²­ ë°›ìŒ: {request.question}")

    """LLMì— ì§ˆë¬¸í•˜ê³  ì‘ë‹µ ë°›ê¸°"""
    # í˜¸ì¶œ ì‹œì‘ ì‹œê°„ ê¸°ë¡
    start_time = datetime.now()
    
    if request.use_chain:
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©
        # answer = chain.invoke({"question": request.question})
        answer = llm.invoke(request.question)
    else:
        # ì§ì ‘ í˜¸ì¶œ
        answer = llm.invoke(request.question)
    
    # ì‘ë‹µ ì™„ë£Œ ì‹œê°„ ê¸°ë¡
    end_time = datetime.now()
    duration = end_time - start_time
    
    print(f"LLM ì±„íŒ… ì‘ë‹µ ë°˜í™˜: {answer}")
    print(f"â±ï¸  í˜¸ì¶œì‹œê°„: {start_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
    print(f"â±ï¸  ì‘ë‹µì‹œê°„: {end_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
    print(f"â±ï¸  ê±¸ë¦°ì‹œê°„: {duration.total_seconds():.3f}ì´ˆ")
    print()
    return ChatResponse(answer=answer)


# ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸
@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """LLM ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ - í† í° ë‹¨ìœ„ë¡œ ì‹¤ì‹œê°„ ì „ì†¡"""
    print(f"LLM ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ìš”ì²­ ë°›ìŒ: {request.question}")
    
    # start_time = None
    # end_time = None
    # duration = None
    
    async def generate():
        # start_time = datetime.now()

        for chunk in llm.stream(request.question):
            print(chunk)
            yield chunk
        
        # end_time = datetime.now()
        # duration = end_time - start_time
    
    
    # print(f"â±ï¸  í˜¸ì¶œì‹œê°„: {start_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
    # print(f"â±ï¸  ì‘ë‹µì‹œê°„: {end_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
    # print(f"â±ï¸  ê±¸ë¦°ì‹œê°„: {duration.total_seconds():.3f}ì´ˆ")
    
    return StreamingResponse(generate(), media_type="text/plain")


# ì§ì ‘ ì‹¤í–‰ ì‹œ
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
